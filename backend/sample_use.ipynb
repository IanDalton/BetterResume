{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_PROMPT = \"\"\"Analista de Marketing, contacto y seguimiento con el cliente, planificaciÃ³n de campaÃ±as, anÃ¡lisis de base de datos de marketing, anÃ¡lisis de mÃ©tricas, creaciÃ³n de dashboards y reportes, estrategia comercial, identificaciÃ³n de oportunidades y tendencias. \"\"\"\n",
    "\n",
    "JOB_PROMPT_SPANISH = \"\"\"Â¿QuiÃ©nes somos?\n",
    "\n",
    "Somos la empresa de tecnologÃ­a y negocios mÃ¡s prestigiosa de IberoamÃ©rica. Nuestra red regional permite responder rÃ¡pidamente a la demanda, con especialistas en cada problemÃ¡tica y aplicando innovaciÃ³n tecnolÃ³gica a cada negocio.\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“¢ Â¿QuÃ© buscamos?\n",
    "\n",
    "Nos encontramos en la bÃºsqueda de Data Scientist.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“¢ Â¿QuÃ© tareas deberÃ¡ hacer el perfil que se sume?\n",
    "\n",
    "Desarrollo con Python y/o R.\n",
    "Conocimiento de librerÃ­as como pandas y sklearn. TambiÃ©n librerÃ­as de visualizaciÃ³n, como matplotlib, pyplot, seaborn.\n",
    "Lenguaje de consulta SQL.\n",
    "Aplicar las siguientes tÃ©cnicas: modelos de regresiÃ³n, clasificaciÃ³n, series temporales, Ã¡rboles, ensambles, redes neuronales, tÃ©cnicas de clustering y reducciÃ³n de dimensionalidad.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Â¿QuÃ© necesitamos? ðŸ’ª\n",
    "\n",
    "\n",
    "\n",
    "2 aÃ±os de experiencia como cientÃ­fico/a de datos.\n",
    "Experiencia en trabajo en modelos predictivos.\n",
    "Experiencia en IA Generativa.\n",
    "Deseable experiencia en metodologÃ­as Ã¡giles.\n",
    "Poseer conocimientos sobre modelos supervisados y no supervisados, evaluaciÃ³n de modelos, balanceo de datos.\n",
    "Tener conocimientos en temas de machine learning.\n",
    "Conocimiento de estadÃ­stica bÃ¡sica aplicada a Ciencia de Datos\n",
    "Capacidad de transmisiÃ³n de conocimientos y presentaciÃ³n de resultados de forma asertiva\n",
    "Conocimiento de buenas prÃ¡cticas de programaciÃ³n\n",
    "Manejo de plataformas Cloud (preferentemente Azure)\n",
    "\n",
    "\n",
    "â™¥ Â¿QuÃ© beneficios te ofrecemos?\n",
    "\n",
    "Licencia extendida para dÃ­as de estudio.\n",
    "DÃ­as de licencia personal para usar como mÃ¡s te guste.\n",
    "DÃ­as de home office.\n",
    "La posibilidad de crecer profesionalmente en un ambiente dinÃ¡mico y flexible.\n",
    "FormaciÃ³n profesional relacionada con el perfil.\n",
    "CapacitaciÃ³n en herramientas de gestiÃ³n, liderazgo y tecnologÃ­as.\n",
    "Descuentos en Universidades e Instituciones TecnolÃ³gicas para MaestrÃ­as, Postgrados y Cursos.\n",
    "Clases de InglÃ©s In company.\n",
    "Prepaga de primer nivel para el grupo familiar.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/idalton/Documents/github/misc/BetterResume/.venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "/Users/idalton/Documents/github/misc/BetterResume/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bot import Bot\n",
    "from resume import WordResumeWriter,LatexResumeWriter\n",
    "from llm.gemini_agent import GeminiAgent\n",
    "from llm.pg_vector_tool import PGVectorTool\n",
    "from models.resume import ResumeOutputFormat\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bot = Bot(writer=WordResumeWriter(),user_id=\"ian_test\", llm=GeminiAgent(output_format=ResumeOutputFormat, tools=[PGVectorTool(db_url=\"postgresql+asyncpg://betterresume_user:betterresume_password@localhost:5432/betterresume_db\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content='Role:\\nYou are BetterResume, an open-source tool that creates high-quality, ATS-optimized resumes.\\n\\nThe user has granted you access to their experience and profile data, stored in a vector database. You can retrieve relevant information by calling PGVectorTool with the argument query: str. This returns the most relevant content to include in the resume (responsibilities, achievements, skills, education, projects).\\n\\nInstructions\\n1. Data Retrieval\\nExtract responsibilities, skills, technologies, and soft skills directly from the job description.\\n\\nMake at least 3 separate PGVectorTool calls for different skills/responsibilities, 4+ queries each, covering:\\n\\nCore required skills & tools\\n\\nIndustry/domain knowledge\\n\\nImpact/metrics-related achievements\\n\\nMerge results, deduplicate, and keep only content explicitly supported by retrieved data. If the database has limited or no experience data, fall back to the job description and any available CSV fields (education, certifications, projects, info) to craft a strong resume without fabricating specific employers.\\n\\n2. Resume Construction Rules\\nExperience Section\\n\\nIf you have sufficient data, include at least 3 experiences (jobs, contracts, or volunteer work), ordered most recent to oldest. If not enough experience is available, omit the Experience section and instead strengthen the Skills and Education sections. You may include a short \"Relevant Projects\" or \"Coursework\" list only if present in the data; do not invent employers or roles.\\n\\nIf date ranges overlap, add a short note: (contract role alongside main position).\\n\\nEvery bullet must follow this structure:\\nAction Verb â†’ Task â†’ Tools/Tech â†’ Result (Metric)\\n\\nKeep bullets 3â€“4 lines max and make them consistent in length and style.\\n\\nShow measurable outcomes where possible; avoid vague adjectives.\\n\\nInclude both hard and soft skills in the context of achievements (e.g., â€œLed cross-functional team of 5â€¦â€).\\n\\nSkills Section\\n\\nMerge related skills into broader categories to reduce redundancy.\\n\\nEach skill line should have:\\nSkill Category â€“ Short, results-oriented description with practical application and measurable impact.\\n\\nAvoid repeating the same tool/term in multiple categories unless context changes.\\n\\nInclude full forms + abbreviations for acronyms on first mention.\\n\\nATS Optimization\\n\\nMatch the language and keywords in the job description (including synonyms where appropriate).\\n\\nPrioritize wording from the Responsibilities and Requirements sections of the job description.\\n\\nEducation Section\\n\\nKeep concise: one line per entry (Institution â€¢ Degree â€¢ Date).\\n\\nPlace at the bottom unless job description requests otherwise.\\n\\n3. Style Enforcement\\nNo vague adjectives without proof (replace â€œexcellentâ€ with â€œreduced processing time by 35%â€). If metrics are unknown, write clear, specific responsibilities without invented numbers.\\n\\nKeep tense consistent: present tense for current roles, past tense for previous ones.\\n\\nNo fabrication of metrics, dates, or tools.\\n\\nEnsure at least one measurable achievement per experience when experiences exist. If no experience data is available, provide a concise, outcome-oriented summary and skills section. Never output apologies or error messages to the user; always return a valid resume JSON.', additional_kwargs={}, response_metadata={}, id='f6703166-cca6-4677-8fec-21af166c7283'),\n",
       "  HumanMessage(content='Â¿QuiÃ©nes somos?\\n\\nSomos la empresa de tecnologÃ­a y negocios mÃ¡s prestigiosa de IberoamÃ©rica. Nuestra red regional permite responder rÃ¡pidamente a la demanda, con especialistas en cada problemÃ¡tica y aplicando innovaciÃ³n tecnolÃ³gica a cada negocio.\\n\\n\\n\\nðŸ“¢ Â¿QuÃ© buscamos?\\n\\nNos encontramos en la bÃºsqueda de Data Scientist.\\n\\n\\n\\n\\n\\nðŸ“¢ Â¿QuÃ© tareas deberÃ¡ hacer el perfil que se sume?\\n\\nDesarrollo con Python y/o R.\\nConocimiento de librerÃ­as como pandas y sklearn. TambiÃ©n librerÃ­as de visualizaciÃ³n, como matplotlib, pyplot, seaborn.\\nLenguaje de consulta SQL.\\nAplicar las siguientes tÃ©cnicas: modelos de regresiÃ³n, clasificaciÃ³n, series temporales, Ã¡rboles, ensambles, redes neuronales, tÃ©cnicas de clustering y reducciÃ³n de dimensionalidad.\\n\\n\\n\\n\\nÂ¿QuÃ© necesitamos? ðŸ’ª\\n\\n\\n\\n2 aÃ±os de experiencia como cientÃ­fico/a de datos.\\nExperiencia en trabajo en modelos predictivos.\\nExperiencia en IA Generativa.\\nDeseable experiencia en metodologÃ­as Ã¡giles.\\nPoseer conocimientos sobre modelos supervisados y no supervisados, evaluaciÃ³n de modelos, balanceo de datos.\\nTener conocimientos en temas de machine learning.\\nConocimiento de estadÃ­stica bÃ¡sica aplicada a Ciencia de Datos\\nCapacidad de transmisiÃ³n de conocimientos y presentaciÃ³n de resultados de forma asertiva\\nConocimiento de buenas prÃ¡cticas de programaciÃ³n\\nManejo de plataformas Cloud (preferentemente Azure)\\n\\n\\nâ™¥ Â¿QuÃ© beneficios te ofrecemos?\\n\\nLicencia extendida para dÃ­as de estudio.\\nDÃ­as de licencia personal para usar como mÃ¡s te guste.\\nDÃ­as de home office.\\nLa posibilidad de crecer profesionalmente en un ambiente dinÃ¡mico y flexible.\\nFormaciÃ³n profesional relacionada con el perfil.\\nCapacitaciÃ³n en herramientas de gestiÃ³n, liderazgo y tecnologÃ­as.\\nDescuentos en Universidades e Instituciones TecnolÃ³gicas para MaestrÃ­as, Postgrados y Cursos.\\nClases de InglÃ©s In company.\\nPrepaga de primer nivel para el grupo familiar.\\n', additional_kwargs={}, response_metadata={}, id='9dbbd190-1c78-48f6-93a2-b71ae858ee44'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'PGVectorTool', 'arguments': '{\"n_results\": 3, \"query\": \"Communication, knowledge sharing, assertive presentation of results, good programming practices, agile methodologies\"}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b3f4d-5036-79e3-bf86-1a778d1fd0a8-0', tool_calls=[{'name': 'PGVectorTool', 'args': {'query': 'Data Scientist responsibilities and achievements', 'n_results': 3}, 'id': '21a998cb-d2e2-4e36-8d23-52db867f7932', 'type': 'tool_call'}, {'name': 'PGVectorTool', 'args': {'n_results': 3, 'query': 'Python, R, pandas, sklearn, SQL, machine learning, regression, classification, time series, clustering, dimensionality reduction, neural networks'}, 'id': '8fac3dd3-6c74-47fc-bd1a-2c3301dd9e69', 'type': 'tool_call'}, {'name': 'PGVectorTool', 'args': {'n_results': 3, 'query': 'Cloud platforms, Azure, data modeling, predictive modeling, Generative AI, supervised and unsupervised models, model evaluation, data balancing, statistical knowledge'}, 'id': '750a656c-eb9c-4027-9d55-079ab4e6aa83', 'type': 'tool_call'}, {'name': 'PGVectorTool', 'args': {'query': 'Data Scientist education and certifications', 'n_results': 3}, 'id': '04d6d0ca-d97f-4e00-b145-f8b6e710a3c3', 'type': 'tool_call'}, {'name': 'PGVectorTool', 'args': {'n_results': 3, 'query': 'Communication, knowledge sharing, assertive presentation of results, good programming practices, agile methodologies'}, 'id': 'c5a663f4-c9af-4aa9-a5d6-55d7b81f51a7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1544, 'output_tokens': 187, 'total_tokens': 1731, 'input_token_details': {'cache_read': 0}}),\n",
       "  ToolMessage(content='[[\"type: job\\\\ncompany: Mouse Soluciones\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/02/2020\\\\nend_date: 01/08/2024\\\\ndescription: Python, SQL, PostgreSQL, R. Designed and implemented custom software solutions using Python, SQL, and Firebase to address client-specific needs. User R to analyze data and process large volumes of information to do introductory data analisys and EDA (exploratory analisys) for a machine learning proyect. Cleaned and processed data using Python and R saving over 250 man hours on cleaning and loading data. Developed a web application using Django and React, enhancing user experience and engagement. Collaborated with cross-functional teams to gather requirements and deliver solutions that met client expectations. Created and maintained documentation for software development processes, ensuring knowledge transfer and project continuity.\", 0.9541815857921677], [\"type: proyect\\\\ncompany: Better Resume\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/03/2025\\\\nend_date: 30/03/2025\\\\ndescription: Python, Vector Database, fastapi. Developed a web app that the user can use to create a resume using LLMs and RAGs to generate a resume tailored to the specific position description. It was built using ChromaDB for the vector database, a locally hosted model and fastapi with a basic javascript frontend. The data is currently stored in a csv so the user can load and modify the data on a familiar format.\", 0.9656683915151041], [\"type: part-time\\\\ncompany: ITBA\\\\nlocation: Argentina\\\\nrole: Data structure\\'s Teachers assistan\\\\nstart_date: 01/03/2023\\\\nend_date: \\\\ndescription: Python, matplotlib, PyQT, Threading, asyncio, requests, aiohttp. Assisted the teacher in the Data Structures course, helping students with their doubts and explaining the theory behind the algorithms. Also helped in the creation of the final project of the courses. The projects were about creating UIs for a goverment database that had to allow for the creation and persistence of data in a local database.\", 0.9695217688478831]]', name='PGVectorTool', id='68fae2db-1f49-4066-9e22-2fae0e7f6859', tool_call_id='21a998cb-d2e2-4e36-8d23-52db867f7932'),\n",
       "  ToolMessage(content='[[\"type: job\\\\ncompany: Mouse Soluciones\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/02/2020\\\\nend_date: 01/08/2024\\\\ndescription: Python, SQL, PostgreSQL, R. Designed and implemented custom software solutions using Python, SQL, and Firebase to address client-specific needs. User R to analyze data and process large volumes of information to do introductory data analisys and EDA (exploratory analisys) for a machine learning proyect. Cleaned and processed data using Python and R saving over 250 man hours on cleaning and loading data. Developed a web application using Django and React, enhancing user experience and engagement. Collaborated with cross-functional teams to gather requirements and deliver solutions that met client expectations. Created and maintained documentation for software development processes, ensuring knowledge transfer and project continuity.\", 0.7713595905066042], [\"type: proyect\\\\ncompany: Better Resume\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/03/2025\\\\nend_date: 30/03/2025\\\\ndescription: Python, Vector Database, fastapi. Developed a web app that the user can use to create a resume using LLMs and RAGs to generate a resume tailored to the specific position description. It was built using ChromaDB for the vector database, a locally hosted model and fastapi with a basic javascript frontend. The data is currently stored in a csv so the user can load and modify the data on a familiar format.\", 0.8198805625506984], [\"type: job\\\\ncompany: Accenture\\\\nlocation: Argentina\\\\nrole: Data Engineer\\\\nstart_date: 01/08/2024\\\\nend_date: \\\\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities, collaborated on the proyect with two other engineers using git and dividing tasks on sprints. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns, saving over 120 man hours on cleaning and loading data.\", 0.8245360861862875]]', name='PGVectorTool', id='8ce0f08f-d6cf-430e-a9de-e7a10ec3a362', tool_call_id='8fac3dd3-6c74-47fc-bd1a-2c3301dd9e69'),\n",
       "  ToolMessage(content='[[\"type: job\\\\ncompany: Accenture\\\\nlocation: Argentina\\\\nrole: Data Engineer\\\\nstart_date: 01/08/2024\\\\nend_date: \\\\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities, collaborated on the proyect with two other engineers using git and dividing tasks on sprints. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns, saving over 120 man hours on cleaning and loading data.\", 0.7479063056362729], [\"type: job\\\\ncompany: Turbodato\\\\nlocation: Argentina\\\\nrole: Data Architect\\\\nstart_date: 01/01/2024\\\\nend_date: 01/12/2024\\\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making. Created automated tasks using azure functions to reduce the time spent on manual tasks by 80%.\", 0.7631667239618235], [\"type: job\\\\ncompany: Mouse Soluciones\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/02/2020\\\\nend_date: 01/08/2024\\\\ndescription: Python, SQL, PostgreSQL, R. Designed and implemented custom software solutions using Python, SQL, and Firebase to address client-specific needs. User R to analyze data and process large volumes of information to do introductory data analisys and EDA (exploratory analisys) for a machine learning proyect. Cleaned and processed data using Python and R saving over 250 man hours on cleaning and loading data. Developed a web application using Django and React, enhancing user experience and engagement. Collaborated with cross-functional teams to gather requirements and deliver solutions that met client expectations. Created and maintained documentation for software development processes, ensuring knowledge transfer and project continuity.\", 0.7677150148043176]]', name='PGVectorTool', id='f367f1a2-0a07-4379-81e5-e1f148580128', tool_call_id='750a656c-eb9c-4027-9d55-079ab4e6aa83'),\n",
       "  ToolMessage(content='[[\"type: job\\\\ncompany: Mouse Soluciones\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/02/2020\\\\nend_date: 01/08/2024\\\\ndescription: Python, SQL, PostgreSQL, R. Designed and implemented custom software solutions using Python, SQL, and Firebase to address client-specific needs. User R to analyze data and process large volumes of information to do introductory data analisys and EDA (exploratory analisys) for a machine learning proyect. Cleaned and processed data using Python and R saving over 250 man hours on cleaning and loading data. Developed a web application using Django and React, enhancing user experience and engagement. Collaborated with cross-functional teams to gather requirements and deliver solutions that met client expectations. Created and maintained documentation for software development processes, ensuring knowledge transfer and project continuity.\", 0.9927739190087885], [\"type: job\\\\ncompany: Accenture\\\\nlocation: Argentina\\\\nrole: Data Engineer\\\\nstart_date: 01/08/2024\\\\nend_date: \\\\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities, collaborated on the proyect with two other engineers using git and dividing tasks on sprints. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns, saving over 120 man hours on cleaning and loading data.\", 0.9938008565305203], [\"type: part-time\\\\ncompany: ITBA\\\\nlocation: Argentina\\\\nrole: Data structure\\'s Teachers assistan\\\\nstart_date: 01/03/2023\\\\nend_date: \\\\ndescription: Python, matplotlib, PyQT, Threading, asyncio, requests, aiohttp. Assisted the teacher in the Data Structures course, helping students with their doubts and explaining the theory behind the algorithms. Also helped in the creation of the final project of the courses. The projects were about creating UIs for a goverment database that had to allow for the creation and persistence of data in a local database.\", 0.9967803383968438]]', name='PGVectorTool', id='79db6bf0-e18d-4ac2-b697-5332e9622aca', tool_call_id='04d6d0ca-d97f-4e00-b145-f8b6e710a3c3'),\n",
       "  ToolMessage(content='[[\"type: job\\\\ncompany: Mouse Soluciones\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/02/2020\\\\nend_date: 01/08/2024\\\\ndescription: Python, SQL, PostgreSQL, R. Designed and implemented custom software solutions using Python, SQL, and Firebase to address client-specific needs. User R to analyze data and process large volumes of information to do introductory data analisys and EDA (exploratory analisys) for a machine learning proyect. Cleaned and processed data using Python and R saving over 250 man hours on cleaning and loading data. Developed a web application using Django and React, enhancing user experience and engagement. Collaborated with cross-functional teams to gather requirements and deliver solutions that met client expectations. Created and maintained documentation for software development processes, ensuring knowledge transfer and project continuity.\", 0.8573626104567271], [\"type: job\\\\ncompany: Accenture\\\\nlocation: Argentina\\\\nrole: Data Engineer\\\\nstart_date: 01/08/2024\\\\nend_date: \\\\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities, collaborated on the proyect with two other engineers using git and dividing tasks on sprints. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns, saving over 120 man hours on cleaning and loading data.\", 0.878506853696764], [\"type: part-time\\\\ncompany: ITBA\\\\nlocation: Argentina\\\\nrole: Data structure\\'s Teachers assistan\\\\nstart_date: 01/03/2023\\\\nend_date: \\\\ndescription: Python, matplotlib, PyQT, Threading, asyncio, requests, aiohttp. Assisted the teacher in the Data Structures course, helping students with their doubts and explaining the theory behind the algorithms. Also helped in the creation of the final project of the courses. The projects were about creating UIs for a goverment database that had to allow for the creation and persistence of data in a local database.\", 0.8823673510076039]]', name='PGVectorTool', id='41323b0e-c2e3-4a04-93b5-b3130bda7efa', tool_call_id='c5a663f4-c9af-4aa9-a5d6-55d7b81f51a7'),\n",
       "  AIMessage(content='', additional_kwargs={'function_call': {'name': 'ResumeOutputFormat', 'arguments': '{\"language\": \"ES\", \"resume_section\": {\"education\": [], \"experience\": [{\"start_date\": \"01/08/2024\", \"location\": \"Argentina\", \"company\": \"Accenture\", \"position\": \"Data Engineer\", \"description\": \"- Desarroll\\\\u00e9 e integr\\\\u00e9 soluciones impulsadas por IA para un gran banco, utilizando LangChain y RAGs para mejorar las capacidades de LLM.\\\\n- Implement\\\\u00e9 el procesamiento de mensajes de voz y consultas de datos estructurados, mejorando la precisi\\\\u00f3n de la respuesta del sistema.\\\\n- Colabor\\\\u00e9 en sprints \\\\u00e1giles, contribuyendo a aumentar la eficiencia y el rendimiento del sistema.\\\\n- Utilic\\\\u00e9 t\\\\u00e9cnicas de data scraping para recopilar informaci\\\\u00f3n para mi base de datos vectorial (chromaDB) y la desplegu\\\\u00e9 en Azure usando GitHub actions con pruebas y CI/CD para garantizar la consistencia en el producto.\\\\n- Trabaj\\\\u00e9 en pipelines de ETL para cargar y generar un sistema para detectar anomal\\\\u00edas en la m\\\\u00e1quina con el fin de prevenir aver\\\\u00edas repentinas, ahorrando m\\\\u00e1s de 120 horas de trabajo humano en la limpieza y carga de datos.\", \"end_date\": \"\"}, {\"description\": \"- Dise\\\\u00f1\\\\u00e9 e implement\\\\u00e9 soluciones de software personalizadas utilizando Python, SQL y Firebase para satisfacer necesidades espec\\\\u00edficas del cliente.\\\\n- Utilic\\\\u00e9 R para analizar datos y procesar grandes vol\\\\u00famenes de informaci\\\\u00f3n para un proyecto de machine learning.\\\\n- Limpi\\\\u00e9 y proces\\\\u00e9 datos utilizando Python y R, ahorrando m\\\\u00e1s de 250 horas de trabajo humano en la limpieza y carga de datos.\\\\n- Desarroll\\\\u00e9 una aplicaci\\\\u00f3n web utilizando Django y React, mejorando la experiencia y el compromiso del usuario.\\\\n- Colabor\\\\u00e9 con equipos multifuncionales para recopilar requisitos y entregar soluciones que cumplieran con las expectativas del cliente.\", \"company\": \"Mouse Soluciones\", \"position\": \"Software Developer\", \"start_date\": \"01/02/2020\", \"location\": \"Argentina\", \"end_date\": \"01/08/2024\"}, {\"start_date\": \"01/03/2023\", \"description\": \"- Asist\\\\u00ed al profesor en el curso de Estructuras de Datos, ayudando a los estudiantes con sus dudas y explicando la teor\\\\u00eda detr\\\\u00e1s de los algoritmos.\\\\n- Ayud\\\\u00e9 en la creaci\\\\u00f3n del proyecto final de los cursos, que consisti\\\\u00f3 en la creaci\\\\u00f3n de interfaces de usuario para una base de datos gubernamental.\", \"location\": \"Argentina\", \"position\": \"Data structure\\'s Teacher Assistant\", \"end_date\": \"\", \"company\": \"ITBA\"}], \"skills\": [{\"description\": \"Experiencia en modelos predictivos, incluyendo regresi\\\\u00f3n, clasificaci\\\\u00f3n, series temporales, \\\\u00e1rboles, ensambles, redes neuronales, clustering y reducci\\\\u00f3n de dimensionalidad. S\\\\u00f3lidos conocimientos en modelos supervisados y no supervisados, evaluaci\\\\u00f3n de modelos y balanceo de datos.\", \"name\": \"Machine Learning\"}, {\"name\": \"Lenguajes de Programaci\\\\u00f3n\", \"description\": \"Desarrollo avanzado en Python y R, con experiencia en librer\\\\u00edas como pandas, sklearn, matplotlib, pyplot y seaborn. Dominio del lenguaje de consulta SQL.\"}, {\"name\": \"Cloud Computing\", \"description\": \"Experiencia en el manejo de plataformas Cloud, con preferencia por Azure. Despliegue de aplicaciones y bases de datos vectoriales utilizando servicios en la nube y herramientas de CI/CD.\"}, {\"name\": \"IA Generativa\", \"description\": \"Experiencia en la aplicaci\\\\u00f3n de t\\\\u00e9cnicas de IA Generativa y modelos como GPT para el desarrollo de soluciones innovadoras. Familiaridad con herramientas como LangChain y RAGs.\"}, {\"name\": \"Metodolog\\\\u00edas y Buenas Pr\\\\u00e1cticas\", \"description\": \"Experiencia en metodolog\\\\u00edas \\\\u00e1giles y conocimiento de buenas pr\\\\u00e1cticas de programaci\\\\u00f3n. Capacidad para la transmisi\\\\u00f3n de conocimientos y presentaci\\\\u00f3n asertiva de resultados.\"}], \"professional_summary\": \"Ingeniero de datos con experiencia en el desarrollo e integraci\\\\u00f3n de soluciones impulsadas por IA, mejora de la precisi\\\\u00f3n de los sistemas y colaboraci\\\\u00f3n en sprints \\\\u00e1giles. Experiencia en el uso de t\\\\u00e9cnicas de data scraping, bases de datos vectoriales y despliegue en plataformas en la nube como Azure. Con una s\\\\u00f3lida formaci\\\\u00f3n en Python, SQL y JavaScript, he trabajado en pipelines de ETL y en la creaci\\\\u00f3n de sistemas para detectar anomal\\\\u00edas, logrando una reducci\\\\u00f3n significativa del tiempo manual de procesamiento de datos.\", \"title\": \"Data Scientist\"}}'}}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019b3f4d-71aa-7b01-b614-3bb21572df07-0', tool_calls=[{'name': 'ResumeOutputFormat', 'args': {'language': 'ES', 'resume_section': {'education': [], 'experience': [{'start_date': '01/08/2024', 'location': 'Argentina', 'company': 'Accenture', 'position': 'Data Engineer', 'description': '- DesarrollÃ© e integrÃ© soluciones impulsadas por IA para un gran banco, utilizando LangChain y RAGs para mejorar las capacidades de LLM.\\n- ImplementÃ© el procesamiento de mensajes de voz y consultas de datos estructurados, mejorando la precisiÃ³n de la respuesta del sistema.\\n- ColaborÃ© en sprints Ã¡giles, contribuyendo a aumentar la eficiencia y el rendimiento del sistema.\\n- UtilicÃ© tÃ©cnicas de data scraping para recopilar informaciÃ³n para mi base de datos vectorial (chromaDB) y la despleguÃ© en Azure usando GitHub actions con pruebas y CI/CD para garantizar la consistencia en el producto.\\n- TrabajÃ© en pipelines de ETL para cargar y generar un sistema para detectar anomalÃ­as en la mÃ¡quina con el fin de prevenir averÃ­as repentinas, ahorrando mÃ¡s de 120 horas de trabajo humano en la limpieza y carga de datos.', 'end_date': ''}, {'description': '- DiseÃ±Ã© e implementÃ© soluciones de software personalizadas utilizando Python, SQL y Firebase para satisfacer necesidades especÃ­ficas del cliente.\\n- UtilicÃ© R para analizar datos y procesar grandes volÃºmenes de informaciÃ³n para un proyecto de machine learning.\\n- LimpiÃ© y procesÃ© datos utilizando Python y R, ahorrando mÃ¡s de 250 horas de trabajo humano en la limpieza y carga de datos.\\n- DesarrollÃ© una aplicaciÃ³n web utilizando Django y React, mejorando la experiencia y el compromiso del usuario.\\n- ColaborÃ© con equipos multifuncionales para recopilar requisitos y entregar soluciones que cumplieran con las expectativas del cliente.', 'company': 'Mouse Soluciones', 'position': 'Software Developer', 'start_date': '01/02/2020', 'location': 'Argentina', 'end_date': '01/08/2024'}, {'start_date': '01/03/2023', 'description': '- AsistÃ­ al profesor en el curso de Estructuras de Datos, ayudando a los estudiantes con sus dudas y explicando la teorÃ­a detrÃ¡s de los algoritmos.\\n- AyudÃ© en la creaciÃ³n del proyecto final de los cursos, que consistiÃ³ en la creaciÃ³n de interfaces de usuario para una base de datos gubernamental.', 'location': 'Argentina', 'position': \"Data structure's Teacher Assistant\", 'end_date': '', 'company': 'ITBA'}], 'skills': [{'description': 'Experiencia en modelos predictivos, incluyendo regresiÃ³n, clasificaciÃ³n, series temporales, Ã¡rboles, ensambles, redes neuronales, clustering y reducciÃ³n de dimensionalidad. SÃ³lidos conocimientos en modelos supervisados y no supervisados, evaluaciÃ³n de modelos y balanceo de datos.', 'name': 'Machine Learning'}, {'name': 'Lenguajes de ProgramaciÃ³n', 'description': 'Desarrollo avanzado en Python y R, con experiencia en librerÃ­as como pandas, sklearn, matplotlib, pyplot y seaborn. Dominio del lenguaje de consulta SQL.'}, {'name': 'Cloud Computing', 'description': 'Experiencia en el manejo de plataformas Cloud, con preferencia por Azure. Despliegue de aplicaciones y bases de datos vectoriales utilizando servicios en la nube y herramientas de CI/CD.'}, {'name': 'IA Generativa', 'description': 'Experiencia en la aplicaciÃ³n de tÃ©cnicas de IA Generativa y modelos como GPT para el desarrollo de soluciones innovadoras. Familiaridad con herramientas como LangChain y RAGs.'}, {'name': 'MetodologÃ­as y Buenas PrÃ¡cticas', 'description': 'Experiencia en metodologÃ­as Ã¡giles y conocimiento de buenas prÃ¡cticas de programaciÃ³n. Capacidad para la transmisiÃ³n de conocimientos y presentaciÃ³n asertiva de resultados.'}], 'professional_summary': 'Ingeniero de datos con experiencia en el desarrollo e integraciÃ³n de soluciones impulsadas por IA, mejora de la precisiÃ³n de los sistemas y colaboraciÃ³n en sprints Ã¡giles. Experiencia en el uso de tÃ©cnicas de data scraping, bases de datos vectoriales y despliegue en plataformas en la nube como Azure. Con una sÃ³lida formaciÃ³n en Python, SQL y JavaScript, he trabajado en pipelines de ETL y en la creaciÃ³n de sistemas para detectar anomalÃ­as, logrando una reducciÃ³n significativa del tiempo manual de procesamiento de datos.', 'title': 'Data Scientist'}}, 'id': '4aae7c3a-6c53-49a9-8895-73c8f8e99cd0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 4701, 'output_tokens': 942, 'total_tokens': 5643, 'input_token_details': {'cache_read': 0}}),\n",
       "  ToolMessage(content='Returning structured response: language=\\'ES\\' resume_section=ResumeSection(title=\\'Data Scientist\\', professional_summary=\\'Ingeniero de datos con experiencia en el desarrollo e integraciÃ³n de soluciones impulsadas por IA, mejora de la precisiÃ³n de los sistemas y colaboraciÃ³n en sprints Ã¡giles. Experiencia en el uso de tÃ©cnicas de data scraping, bases de datos vectoriales y despliegue en plataformas en la nube como Azure. Con una sÃ³lida formaciÃ³n en Python, SQL y JavaScript, he trabajado en pipelines de ETL y en la creaciÃ³n de sistemas para detectar anomalÃ­as, logrando una reducciÃ³n significativa del tiempo manual de procesamiento de datos.\\', experience=[JobExperience(position=\\'Data Engineer\\', company=\\'Accenture\\', location=\\'Argentina\\', start_date=\\'01/08/2024\\', end_date=\\'\\', description=\\'- DesarrollÃ© e integrÃ© soluciones impulsadas por IA para un gran banco, utilizando LangChain y RAGs para mejorar las capacidades de LLM.\\\\n- ImplementÃ© el procesamiento de mensajes de voz y consultas de datos estructurados, mejorando la precisiÃ³n de la respuesta del sistema.\\\\n- ColaborÃ© en sprints Ã¡giles, contribuyendo a aumentar la eficiencia y el rendimiento del sistema.\\\\n- UtilicÃ© tÃ©cnicas de data scraping para recopilar informaciÃ³n para mi base de datos vectorial (chromaDB) y la despleguÃ© en Azure usando GitHub actions con pruebas y CI/CD para garantizar la consistencia en el producto.\\\\n- TrabajÃ© en pipelines de ETL para cargar y generar un sistema para detectar anomalÃ­as en la mÃ¡quina con el fin de prevenir averÃ­as repentinas, ahorrando mÃ¡s de 120 horas de trabajo humano en la limpieza y carga de datos.\\'), JobExperience(position=\\'Software Developer\\', company=\\'Mouse Soluciones\\', location=\\'Argentina\\', start_date=\\'01/02/2020\\', end_date=\\'01/08/2024\\', description=\\'- DiseÃ±Ã© e implementÃ© soluciones de software personalizadas utilizando Python, SQL y Firebase para satisfacer necesidades especÃ­ficas del cliente.\\\\n- UtilicÃ© R para analizar datos y procesar grandes volÃºmenes de informaciÃ³n para un proyecto de machine learning.\\\\n- LimpiÃ© y procesÃ© datos utilizando Python y R, ahorrando mÃ¡s de 250 horas de trabajo humano en la limpieza y carga de datos.\\\\n- DesarrollÃ© una aplicaciÃ³n web utilizando Django y React, mejorando la experiencia y el compromiso del usuario.\\\\n- ColaborÃ© con equipos multifuncionales para recopilar requisitos y entregar soluciones que cumplieran con las expectativas del cliente.\\'), JobExperience(position=\"Data structure\\'s Teacher Assistant\", company=\\'ITBA\\', location=\\'Argentina\\', start_date=\\'01/03/2023\\', end_date=\\'\\', description=\\'- AsistÃ­ al profesor en el curso de Estructuras de Datos, ayudando a los estudiantes con sus dudas y explicando la teorÃ­a detrÃ¡s de los algoritmos.\\\\n- AyudÃ© en la creaciÃ³n del proyecto final de los cursos, que consistiÃ³ en la creaciÃ³n de interfaces de usuario para una base de datos gubernamental.\\')], skills=[Skill(name=\\'Machine Learning\\', description=\\'Experiencia en modelos predictivos, incluyendo regresiÃ³n, clasificaciÃ³n, series temporales, Ã¡rboles, ensambles, redes neuronales, clustering y reducciÃ³n de dimensionalidad. SÃ³lidos conocimientos en modelos supervisados y no supervisados, evaluaciÃ³n de modelos y balanceo de datos.\\'), Skill(name=\\'Lenguajes de ProgramaciÃ³n\\', description=\\'Desarrollo avanzado en Python y R, con experiencia en librerÃ­as como pandas, sklearn, matplotlib, pyplot y seaborn. Dominio del lenguaje de consulta SQL.\\'), Skill(name=\\'Cloud Computing\\', description=\\'Experiencia en el manejo de plataformas Cloud, con preferencia por Azure. Despliegue de aplicaciones y bases de datos vectoriales utilizando servicios en la nube y herramientas de CI/CD.\\'), Skill(name=\\'IA Generativa\\', description=\\'Experiencia en la aplicaciÃ³n de tÃ©cnicas de IA Generativa y modelos como GPT para el desarrollo de soluciones innovadoras. Familiaridad con herramientas como LangChain y RAGs.\\'), Skill(name=\\'MetodologÃ­as y Buenas PrÃ¡cticas\\', description=\\'Experiencia en metodologÃ­as Ã¡giles y conocimiento de buenas prÃ¡cticas de programaciÃ³n. Capacidad para la transmisiÃ³n de conocimientos y presentaciÃ³n asertiva de resultados.\\')], education=[])', name='ResumeOutputFormat', id='40b2e44a-a6d0-4adb-bb06-f33aa49f8da3', tool_call_id='4aae7c3a-6c53-49a9-8895-73c8f8e99cd0')],\n",
       " 'user_id': 'ian_test'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from llm.state import State\n",
    "s = State(messages=[SystemMessage(bot.llm.JOB_PROMPT),HumanMessage(JOB_PROMPT_SPANISH)], user_id=\"ian_test\")\n",
    "res = await bot.llm.ainvoke(s)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'structured_response'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstructured_response\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 'structured_response'"
     ]
    }
   ],
   "source": [
    "res[\"structured_response\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
