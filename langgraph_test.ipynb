{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = CSVLoader(\n",
    "    file_path=\"jobs.csv\"\n",
    ")\n",
    "data = loader.load()\n",
    "from langchain_openai.chat_models.base import ChatOpenAI\n",
    "\n",
    "from langchain_core.messages import HumanMessage,AIMessage,SystemMessage,BaseMessage\n",
    "llm = ChatOpenAI(\n",
    "            base_url=\"http://localhost:1234/v1\",\n",
    "            api_key=\"not_needed\",\n",
    "            model=\"gemma-2-27b-it\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Documents added successfully.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "from langchain.tools import BaseTool\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from pydantic import Field, PrivateAttr\n",
    "\n",
    "class ChromaDBTool(BaseTool):\n",
    "    name: str = \"ChromaDBTool\"\n",
    "    description: str = (\n",
    "        \"A tool that interfaces with a ChromaDB database to retrieve information. \"\n",
    "        \"It initializes a connection to a specified collection and runs queries against it.\"\n",
    "    )\n",
    "    collection_name: str = Field(default=\"default_collection\")\n",
    "    persist_directory: str = Field(default=\"./chroma_db\")\n",
    "\n",
    "    # Declare private attributes for runtime-only properties.\n",
    "    _client: Any = PrivateAttr()\n",
    "    _collection: Any = PrivateAttr()\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._client = chromadb.Client(Settings(persist_directory=self.persist_directory))\n",
    "        try:\n",
    "            self._collection = self._client.get_collection(name=self.collection_name)\n",
    "        except Exception as e:\n",
    "            self._collection = self._client.create_collection(name=self.collection_name)\n",
    "      \n",
    "    def add_document(self, document: str, id: str):\n",
    "        \"\"\"\n",
    "        Add a document to the ChromaDB collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._collection.add(documents=[document], ids=[id])\n",
    "            return \"Document added successfully.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error adding document: {e}\"\n",
    "    def add_documents(self, documents: list, ids: list):\n",
    "        \"\"\"\n",
    "        Add multiple documents to the ChromaDB collection.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self._collection.add(documents=documents, ids=ids)\n",
    "            return \"Documents added successfully.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error adding documents: {e}\"\n",
    "    def _run(self, query: str, **kwargs):\n",
    "        print(\"chromadb\")\n",
    "        \"\"\"\n",
    "        Synchronous method to query the ChromaDB collection.\n",
    "        Expects a text query and returns the query results.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = self._collection.query(query_texts=[query],n_results=2)\n",
    "            return list(zip(results[\"documents\"][0],results[\"distances\"][0]))\n",
    "        except Exception as e:\n",
    "            return f\"Error querying the database: {e}\"\n",
    "\n",
    "    async def _arun(self, query: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Asynchronous version of the _run method.\n",
    "        Currently, this calls the synchronous version for simplicity.\n",
    "        \"\"\"\n",
    "        return self._run(query, **kwargs)\n",
    "tool = ChromaDBTool()\n",
    "tool.add_documents([d.page_content for d in data], [str(i) for i in range(len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chromadb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"type: contract\\ncompany: Somos Feed\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/07/2024\\nend_date: 01/12/2024\\ndescription: Python, Django, Javascript, SQLite. Designed and built the backend infrastructure on Django for the company to automate the process of data gathering from all its different providers. Generated the ETL pipeline to load the information onto the database from google sheet files and created the machine learning algorithm to be trained weekly on the previous week's data to estimate sales and reduce waste, genrating up to a 68% of waste reduction and increasing the benefits of the company by 30%. The solutiopn also freed time of other data scientists in the team that were previously just cleaning data.\",\n",
       "  1.5428789854049683),\n",
       " ('type: non-profit\\ncompany: CEITBA\\nlocation: Argentina\\nrole: Lead Developer\\nstart_date: 01/03/2021\\nend_date: 01/03/2025\\ndescription: Python, Javascript, React, Dart, Flutter. Led a team of five in developing a Flutter-based mobile application for CEITBA, utilizing Firebase and Google Cloud for scalability. Automated administrative tasks with AI-based tools, optimizing student registrations, withdrawals, and query responses.',\n",
       "  1.5849837064743042)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool.invoke(\"Java\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_with_tools = llm.bind_tools([tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x141b83924b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    # Messages have the type \"list\". The `add_messages` function\n",
    "    # in the annotation defines how this state key should be updated\n",
    "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print('chatbot message')\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x141b83924b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        print('tools message')\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            print(tool_call[\"args\"])\n",
    "            tool_result = self.tools_by_name[\"ChromaDBTool\"].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            print(tool_result)\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=[tool])\n",
    "\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ChromaDBTool': ChromaDBTool()}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_node.tools_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(\n",
    "        state:State\n",
    "):\n",
    "    if isinstance(state, list):\n",
    "        message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(\"No message found in input\")\n",
    "    if hasattr(message, \"tool_calls\") and len(message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_conditional_edges(\"chatbot\", route_tools ,{\"tools\": \"tools\", END: END})\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot message\n",
      "tools message\n",
      "{'query': 'Data Engineer experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.2102479934692383), ('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.2190603017807007)]\n",
      "{'query': 'SQL experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.3872443437576294), ('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.3885654211044312)]\n",
      "{'query': 'NoSQL experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.2850463390350342), ('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.3066861629486084)]\n",
      "{'query': 'API experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.336132526397705), ('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.417433738708496)]\n",
      "{'query': 'Big Data experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.1898853778839111), ('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.2142202854156494)]\n",
      "{'query': 'Apache Spark experience'}\n",
      "chromadb\n",
      "[(\"type: contract\\ncompany: Somos Feed\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/07/2024\\nend_date: 01/12/2024\\ndescription: Python, Django, Javascript, SQLite. Designed and built the backend infrastructure on Django for the company to automate the process of data gathering from all its different providers. Generated the ETL pipeline to load the information onto the database from google sheet files and created the machine learning algorithm to be trained weekly on the previous week's data to estimate sales and reduce waste, genrating up to a 68% of waste reduction and increasing the benefits of the company by 30%. The solutiopn also freed time of other data scientists in the team that were previously just cleaning data.\", 1.288055658340454), ('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.3272736072540283)]\n",
      "{'query': 'Hadoop experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.0763795375823975), (\"type: contract\\ncompany: Somos Feed\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/07/2024\\nend_date: 01/12/2024\\ndescription: Python, Django, Javascript, SQLite. Designed and built the backend infrastructure on Django for the company to automate the process of data gathering from all its different providers. Generated the ETL pipeline to load the information onto the database from google sheet files and created the machine learning algorithm to be trained weekly on the previous week's data to estimate sales and reduce waste, genrating up to a 68% of waste reduction and increasing the benefits of the company by 30%. The solutiopn also freed time of other data scientists in the team that were previously just cleaning data.\", 1.171142816543579)]\n",
      "{'query': 'Power BI experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.355983853340149), ('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.444392204284668)]\n",
      "{'query': 'Tableau experience'}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 1.3360117673873901), ('type: job\\ncompany: Accenture\\nlocation: Argentina\\nrole: Data Engineer\\nstart_date: 01/08/2024\\nend_date: \\ndescription: Python, SQL, Javascript, Google sheets. Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns', 1.4218132495880127)]\n",
      "chatbot message\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"You are BetterResume, an open-source tool that helps users create the best possible resumes optimized for ATS AI scanners.  \n",
    "\n",
    "The user has granted you access to their full job experience, which is stored in a vector database. You can retrieve relevant information from this database by calling `ChromaDBTool` with the argument `query: str`. This will return the most relevant job experience for inclusion in the resume.  \n",
    "\n",
    "**Instructions:**  \n",
    "1. **Make at least one call to `ChromaDBTool`** to retrieve relevant experience.  \n",
    "2. **Make multiple tool calls (at least 4!) according to the different skills asked for in the description** to gather more data.  \n",
    "3. **Wait for the tool's response(s), then format and return the information as a structured JSON object.**  \n",
    "4. **Extract skills, languages, and technologies ONLY if they are explicitly mentioned in the retrieved job descriptions.**  \n",
    "5. **Include at least 3 experiences in the resume output.** It doesnt have to be a job, can be other stuf like contract, volunteer, etc.\n",
    "6. **Ensure the output follows this JSON format:**  \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"language\": \"ES/EN/FR/DE/IT/PT\",\n",
    "  \"resume_section\": {\n",
    "    \"title\": \"Title describing the job that the user is applying for\",\n",
    "    \"experience\": [\n",
    "      {\n",
    "        \"position\": \"Job Title\",\n",
    "        \"company\": \"Company Name\",\n",
    "        \"location\": \"Location\",\n",
    "        \"start_date\": \"Month Year\",\n",
    "        \"end_date\": \"Month Year or Present\",\n",
    "        \"description\": \"Detailed job description and achievements.\"\n",
    "      }\n",
    "    ],\n",
    "    \"skills\": {\n",
    "      \"languages\": [\"List of programming languages\"],\n",
    "      \"databases\": [\"List of databases\"],\n",
    "      \"tools_and_technologies\": [\"List of tools, frameworks, and methodologies\"]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "Do not include additional text, explanations, or formatting outside of the JSON output.\n",
    "\n",
    "Change the experience description if you see fit, but keep the main points as long as they are relevant to the job description.\n",
    "\n",
    "Change the language of the description to match the language of the job description.\n",
    "\n",
    "Do not include languages that are not implied by the database query.\n",
    "\n",
    "Ensure consistency in date formatting and job descriptions to maintain a professional resume output.\n",
    "\n",
    "If you understand, proceed with handling the user request.\n",
    "\"\"\"\n",
    "\n",
    "JOB_PROMPT = \"\"\"About the job\n",
    "Job Description: Azure & SQL Data Engineer\n",
    "\n",
    " \n",
    "\n",
    "As a Data Engineer, you will be responsible for designing, implementing, and maintaining data solutions with a strong focus on SQL databases. You will collaborate with cross-functional teams to gather requirements, design efficient and robust SQL queries, and ensure data integrity and security.\n",
    "\n",
    " \n",
    "\n",
    "Responsibilities:\n",
    "\n",
    "\n",
    "\n",
    "Â· Collaborate with stakeholders to gather data requirements and translate them into technical specifications.\n",
    "\n",
    "Â· Build and optimize SQL queries for data ingestion, transformation, and storage.\n",
    "\n",
    "Â· Utilize Azure Data Factory for data orchestration and workflow automation.\n",
    "\n",
    "Â· Monitor and troubleshoot SQL databases to ensure performance and availability.\n",
    "\n",
    "Â· Work closely with data scientists and analysts to support their data needs through optimized SQL queries and reporting tools.\n",
    "\n",
    "Â· Stay up-to-date with the latest trends and advancements in SQL data engineering and recommend best practices to enhance data solutions.\n",
    "\n",
    " \n",
    "\n",
    "Qualifications:\n",
    "\n",
    "\n",
    "\n",
    "Â· 3+ years of proven experience as a data engineer with a strong focus on SQL.\n",
    "\n",
    "Â· Strong expertise in C#, SQL and extensive experience with data modeling and database design.\n",
    "\n",
    "Â· Excellent problem-solving and analytical skills.\n",
    "\n",
    "Â· Knowledge of data governance and data security principles.\n",
    "\n",
    "Â· Strong communication and collaboration abilities.\n",
    "\n",
    "Â· It is desired familiarity with Power Apps.\n",
    "\n",
    "Â· Ideally, but not mandatory, familiarity with Azure data services such as Azure SQL Database, Azure Data Factory, etc.\"\"\"\n",
    "\n",
    "JOB_PROMPT_SPANISH = \"\"\"\n",
    "Â¿Te apasionan los desafÃ­os y tu objetivo es dar la milla extra siempre? ðŸš€\n",
    "\n",
    "\n",
    "\n",
    "Si tienes ganas de trabajar junto a un gran equipo y afrontar nuevos retos, esta es tu oportunidad!\n",
    "\n",
    "\n",
    "\n",
    "Estamos en la bÃºsqueda de un/a Data Engineer\n",
    "\n",
    "\n",
    "\n",
    "âœ” Â¿QuÃ© conocimientos/experiencia necesitas?\n",
    "\n",
    "\n",
    "\n",
    "Estudiantes promediando o avanzados en carreras universitarias o terciarias de IngenierÃ­a en Sistemas, Licenciaturas o afines o idÃ³neo.\n",
    "Experiencia trabajando con base de datos (SQL y No SQL) y/o en el Ã¡rea Big Data.\n",
    "Conocimiento intermedio de APIs (consulta y creaciÃ³n)\n",
    "Conocimiento intermedio de base de datos (SQL y NO SQL)\n",
    "Experiencia como Full Stack DBA\n",
    "Conocimiento y experiencia en herramientas de visualizaciÃ³n (Power BI, Qlik Sence, Microstrategy, Tableau, R/ Phyton)\n",
    "Conocimiento bÃ¡sico de framework (Apache Spark, Hadoop\n",
    "\n",
    "\n",
    "âž¡Si estÃ¡s en la bÃºsqueda de un cambio en tu carrera que sume a tu perfil profesional ven a trabajar a nuestro equipo!\n",
    "\n",
    "\n",
    "\n",
    "ContÃ¡ctanos!\"\"\"\n",
    "\n",
    "response = graph.invoke({\"messages\": [{\"role\": \"system\", \"content\": PROMPT},\n",
    "                           {\"role\": \"user\", \"content\": JOB_PROMPT_SPANISH}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "  \"language\": \"ES\",\n",
       "  \"resume_section\": {\n",
       "    \"title\": \"Data Engineer\",\n",
       "    \"experience\": [\n",
       "      {\n",
       "        \"position\": \"Data Engineer\",\n",
       "        \"company\": \"Accenture\",\n",
       "        \"location\": \"Argentina\",\n",
       "        \"start_date\": \"08/2024\",\n",
       "        \"end_date\": \"Present\",\n",
       "        \"description\": \"Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns\"\n",
       "      },\n",
       "     {\n",
       "        \"position\": \"Data Architect\",\n",
       "        \"company\": \"Turbodato\",\n",
       "        \"location\": \"Argentina\",\n",
       "        \"start_date\": \"01/2024\",\n",
       "        \"end_date\": \"12/2024\",\n",
       "        \"description\": \"Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.\"\n",
       "      },\n",
       "      {\n",
       "        \"position\": \"Data Architect\",\n",
       "        \"company\": \"Somos Feed\",\n",
       "        \"location\": \"Argentina\",\n",
       "        \"start_date\": \"07/2024\",\n",
       "        \"end_date\": \"12/2024\",\n",
       "        \"description\": \"Designed and built the backend infrastructure on Django for the company to automate the process of data gathering from all its different providers. Generated the ETL pipeline to load the information onto the database from google sheet files and created the machine learning algorithm to be trained weekly on the previous week's data to estimate sales and reduce waste, generating up to a 68% of waste reduction and increasing the benefits of the company by 30%. The solution also freed time of other data scientists in the team that were previously just cleaning data.\"\n",
       "      }\n",
       "    ],\n",
       "    \"skills\": {\n",
       "      \"languages\": [\"Python\", \"SQL\", \"Javascript\"],\n",
       "      \"databases\": [\"SQLite\", \"Azure Database\"],\n",
       "      \"tools_and_technologies\": [\"Django\", \"LangChain\", \"RAGs\", \"Power BI\", \"Google Sheets\", \"chromaDB\", \"GitHub Actions\", \"CI/CD\"]\n",
       "    }\n",
       "  }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = response[\"messages\"][-1].content\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatbot message\n",
      "tools message\n",
      "{'query': 'What databases and tools does the candidate have experience with?', 'kwargs': {}}\n",
      "chromadb\n",
      "[('type: job\\ncompany: Turbodato\\nlocation: Argentina\\nrole: Data Architect\\nstart_date: 01/01/2024\\nend_date: 01/12/2024\\ndescription: Python, SQL, Azure Database. Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.', 0.9016132950782776), ('type: job\\ncompany: Better Resume\\nlocation: Argentina\\nrole: Software Developer\\nstart_date: 01/03/2025\\nend_date: \\ndescription: Python, Vector Database, fastapi. Developed a web app that the user can use to create a resume using LLMs and RAGs to generate a resume tailored to the specific position description. It was built using ChromaDB for the vector database, a locally hosted model and fastapi with a basic javascript frontend. The data is currently stored in a csv so the user can load and modify the data on a familiar format.', 0.9900636672973633)]\n",
      "chatbot message\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [SystemMessage(content='\\nÂ¿Te apasionan los desafÃ­os y tu objetivo es dar la milla extra siempre? ðŸš€\\n\\n\\n\\nSi tienes ganas de trabajar junto a un gran equipo y afrontar nuevos retos, esta es tu oportunidad!\\n\\n\\n\\nEstamos en la bÃºsqueda de un/a Data Engineer\\n\\n\\n\\nâœ” Â¿QuÃ© conocimientos/experiencia necesitas?\\n\\n\\n\\nEstudiantes promediando o avanzados en carreras universitarias o terciarias de IngenierÃ­a en Sistemas, Licenciaturas o afines o idÃ³neo.\\nExperiencia trabajando con base de datos (SQL y No SQL) y/o en el Ã¡rea Big Data.\\nConocimiento intermedio de APIs (consulta y creaciÃ³n)\\nConocimiento intermedio de base de datos (SQL y NO SQL)\\nExperiencia como Full Stack DBA\\nConocimiento y experiencia en herramientas de visualizaciÃ³n (Power BI, Qlik Sence, Microstrategy, Tableau, R/ Phyton)\\nConocimiento bÃ¡sico de framework (Apache Spark, Hadoop\\n\\n\\nâž¡Si estÃ¡s en la bÃºsqueda de un cambio en tu carrera que sume a tu perfil profesional ven a trabajar a nuestro equipo!\\n\\n\\n\\nContÃ¡ctanos!', additional_kwargs={}, response_metadata={}, id='24e3b5eb-b682-4a29-b48b-dff3c66a56fd'),\n",
       "  HumanMessage(content='```json\\n{\\n  \"language\": \"ES\",\\n  \"resume_section\": {\\n    \"title\": \"Data Engineer\",\\n    \"experience\": [\\n      {\\n        \"position\": \"Data Engineer\",\\n        \"company\": \"Accenture\",\\n        \"location\": \"Argentina\",\\n        \"start_date\": \"08/2024\",\\n        \"end_date\": \"Present\",\\n        \"description\": \"Developed and integrated AI-driven solutions for a large bank, utilizing LangChain and RAGs to enhance LLM capabilities. Implemented voice message processing and structured data queries, improving system response accuracy. Collaborated in Agile sprints, contributing to increased system efficiency and performance. Used Data scraping techniques to gather information for my vector database (chromaDB) and deployed it on Azure using github actions with testing and CI/CD to ensure consistency on the product. Also worked on ETL pipelines to upload and generate a system to detect anomalies in the machine in order to prevent sudden breakdowns\"\\n      },\\n     {\\n        \"position\": \"Data Architect\",\\n        \"company\": \"Turbodato\",\\n        \"location\": \"Argentina\",\\n        \"start_date\": \"01/2024\",\\n        \"end_date\": \"12/2024\",\\n        \"description\": \"Designed and built the startupâ€™s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like ItaÃº Bank and Cebra JugueterÃ­as. Created interactive dashboards using Power BI, enabling real-time decision-making.\"\\n      },\\n      {\\n        \"position\": \"Data Architect\",\\n        \"company\": \"Somos Feed\",\\n        \"location\": \"Argentina\",\\n        \"start_date\": \"07/2024\",\\n        \"end_date\": \"12/2024\",\\n        \"description\": \"Designed and built the backend infrastructure on Django for the company to automate the process of data gathering from all its different providers. Generated the ETL pipeline to load the information onto the database from google sheet files and created the machine learning algorithm to be trained weekly on the previous week\\'s data to estimate sales and reduce waste, generating up to a 68% of waste reduction and increasing the benefits of the company by 30%. The solution also freed time of other data scientists in the team that were previously just cleaning data.\"\\n      }\\n    ],\\n    \"skills\": {\\n      \"languages\": [\"Python\", \"SQL\", \"Javascript\"],\\n      \"databases\": [\"SQLite\", \"Azure Database\"],\\n      \"tools_and_technologies\": [\"Django\", \"LangChain\", \"RAGs\", \"Power BI\", \"Google Sheets\", \"chromaDB\", \"GitHub Actions\", \"CI/CD\"]\\n    }\\n  }\\n}\\n```', additional_kwargs={}, response_metadata={}, id='6551bdea-b720-4e70-999c-e887fa00902c'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '226787114', 'function': {'arguments': '{\"query\":\"What databases and tools does the candidate have experience with?\",\"kwargs\":{}}', 'name': 'ChromaDBTool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 1249, 'total_tokens': 1295, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma-2-27b-it', 'system_fingerprint': 'gemma-2-27b-it', 'id': 'chatcmpl-nff4z3uyo51x0b04o65b9', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7a017fc7-4773-499f-9d8d-564f64c577ab-0', tool_calls=[{'name': 'ChromaDBTool', 'args': {'query': 'What databases and tools does the candidate have experience with?', 'kwargs': {}}, 'id': '226787114', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1249, 'output_tokens': 46, 'total_tokens': 1295, 'input_token_details': {}, 'output_token_details': {}}),\n",
       "  ToolMessage(content='[[\"type: job\\\\ncompany: Turbodato\\\\nlocation: Argentina\\\\nrole: Data Architect\\\\nstart_date: 01/01/2024\\\\nend_date: 01/12/2024\\\\ndescription: Python, SQL, Azure Database. Designed and built the startup\\\\u2019s backend infrastructure with a focus on scalability and automation. Developed web scraping bots to collect competitive business intelligence for clients like Ita\\\\u00fa Bank and Cebra Jugueter\\\\u00edas. Created interactive dashboards using Power BI, enabling real-time decision-making.\", 0.9016132950782776], [\"type: job\\\\ncompany: Better Resume\\\\nlocation: Argentina\\\\nrole: Software Developer\\\\nstart_date: 01/03/2025\\\\nend_date: \\\\ndescription: Python, Vector Database, fastapi. Developed a web app that the user can use to create a resume using LLMs and RAGs to generate a resume tailored to the specific position description. It was built using ChromaDB for the vector database, a locally hosted model and fastapi with a basic javascript frontend. The data is currently stored in a csv so the user can load and modify the data on a familiar format.\", 0.9900636672973633]]', name='ChromaDBTool', id='d63c17b1-ced2-4f73-bd89-a8b782980bfa', tool_call_id='226787114'),\n",
       "  AIMessage(content='I am sorry but I cannot use that information as it looks like a response from a tool, not available in my tools list. Please make sure to follow the format and only call tools listed in AVAILABLE TOOLS.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 1615, 'total_tokens': 1657, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gemma-2-27b-it', 'system_fingerprint': 'gemma-2-27b-it', 'id': 'chatcmpl-zyaigxzqywbok57gy7h5pj', 'finish_reason': 'stop', 'logprobs': None}, id='run-bf1f1491-b6e8-4c34-b942-0578a9a54d3f-0', usage_metadata={'input_tokens': 1615, 'output_tokens': 42, 'total_tokens': 1657, 'input_token_details': {}, 'output_token_details': {}})]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LANGUAGE_PROMPT = \"\"\"\n",
    "    Keep the format of the message (json) but translate only the job description to the specified language that is on the language section of the json message.\n",
    "\"\"\"\n",
    "\n",
    "texto = graph.invoke({\"messages\":[{\"role\": \"system\", \"content\": LANGUAGE_PROMPT},\n",
    "                          {\"role\": \"user\", \"content\": text}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texto' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m Markdown(\u001b[43mtexto\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texto' is not defined"
     ]
    }
   ],
   "source": [
    "Markdown(texto[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
